import matchzoo as mz
import torch
import os
import pandas as pd
from typing import Dict, List, Optional
import numpy as np
import argparse
import sys


class ModelEvaluator:
    """L·ªõp ƒë·ªÉ ƒë√°nh gi√° v√† so s√°nh c√°c m√¥ h√¨nh MatchZoo"""
    
    def __init__(self, data_pack_dir: str, batch_size: int = 32, device: str = "cpu"):
        """
        Kh·ªüi t·∫°o evaluator
        
        Args:
            data_pack_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a test_pack.dam
            batch_size: Batch size cho evaluation
            device: Device ƒë·ªÉ ch·∫°y evaluation ('cpu' ho·∫∑c 'cuda')
        """
        self.data_pack_dir = data_pack_dir
        self.batch_size = batch_size
        
        # Ki·ªÉm tra v√† thi·∫øt l·∫≠p device
        if device == "cuda":
            if torch.cuda.is_available():
                self.device = torch.device("cuda")
                print(f"CUDA available. Using GPU: {torch.cuda.get_device_name(0)}")
            else:
                print("Fallback sang CPU.")
                self.device = torch.device("cpu")
        else:
            self.device = torch.device("cpu")
            print(f"üñ•Ô∏è Using CPU device")
        
        self.test_pack_raw = None
        self.results = {}
        
        # T·∫£i test pack
        self._load_test_data()
    
    def _load_test_data(self):
        """T·∫£i d·ªØ li·ªáu test"""
        test_pack_path = os.path.join(self.data_pack_dir, 'test_pack.dam')
        if not os.path.exists(test_pack_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y test_pack.dam trong {self.data_pack_dir}")
        
        self.test_pack_raw = mz.load_data_pack(test_pack_path)
        print(f"ƒê√£ t·∫£i test data: {len(self.test_pack_raw)} samples")
    
    def evaluate_model(self, model_dir: str, model_name: str) -> Dict:

        print(f"\n--- ƒê√°nh gi√° m√¥ h√¨nh: {model_name} ---")
        print(f"Device: {self.device}")
        
        # Ki·ªÉm tra file t·ªìn t·∫°i
        model_path = os.path.join(model_dir, 'model.pt')
        preprocessor_path = os.path.join(model_dir, 'preprocessor')
        
        if not os.path.exists(model_path):
            print(f"Kh√¥ng t√¨m th·∫•y model.pt trong {model_dir}")
            return None
        
        if not os.path.exists(preprocessor_path):
            print(f"Kh√¥ng t√¨m th·∫•y preprocessor trong {model_dir}")
            return None
        
        try:
            # T·∫£i preprocessor
            preprocessor = mz.load_preprocessor(preprocessor_path)
            print("ƒê√£ t·∫£i preprocessor")
            
            # ƒê·ªçc th√¥ng tin embedding dimension t·ª´ saved model
            model_path = os.path.join(model_dir, 'model.pt')
            state_dict = torch.load(model_path, map_location=self.device)
            
            # L·∫•y embedding dimension t·ª´ state_dict
            embedding_dim = state_dict['embedding.weight'].shape[1]
            vocab_size = state_dict['embedding.weight'].shape[0]
            
            # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu test
            test_pack_processed = preprocessor.transform(self.test_pack_raw)
            print("ƒê√£ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu test")
            
            # T·∫°o test dataset
            testset = mz.dataloader.Dataset(
                data_pack=test_pack_processed,
                mode='point',
                batch_size=self.batch_size,
                shuffle=False
            )
            
            # X√°c ƒë·ªãnh lo·∫°i m√¥ h√¨nh ƒë·ªÉ l·∫•y ƒë√∫ng callback
            model_name_lower = model_name.lower().replace('-', '').replace('_', '')
            callback = self._get_model_callback(model_name_lower)

            testloader = mz.dataloader.DataLoader(
                dataset=testset,
                stage='test',
                callback=callback,
                device=self.device
            )
            print("ƒê√£ t·∫°o test loader")
            
            # T·∫°o ranking task v·ªõi metrics ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi MatchZoo
            ranking_task = mz.tasks.Ranking()
            ranking_task.metrics = [
                mz.metrics.MeanAveragePrecision(),  # MAP - Mean Average Precision
                mz.metrics.MeanReciprocalRank(),    # MRR - Mean Reciprocal Rank
                
                # Precision at K (P@K) 
                mz.metrics.Precision(k=1),         # P@1
                mz.metrics.Precision(k=3),         # P@3  
                mz.metrics.Precision(k=5),         # P@5
                mz.metrics.Precision(k=10),        # P@10
                mz.metrics.Precision(k=20),        # P@20
                
                # Normalized Discounted Cumulative Gain (NDCG) 
                mz.metrics.NormalizedDiscountedCumulativeGain(k=1),   # NDCG@1
                mz.metrics.NormalizedDiscountedCumulativeGain(k=3),   # NDCG@3
                mz.metrics.NormalizedDiscountedCumulativeGain(k=5),   # NDCG@5
                mz.metrics.NormalizedDiscountedCumulativeGain(k=10),  # NDCG@10
                mz.metrics.NormalizedDiscountedCumulativeGain(k=20),  # NDCG@20
                
                # Discounted Cumulative Gain (DCG) 
                mz.metrics.DiscountedCumulativeGain(k=1),    # DCG@1
                mz.metrics.DiscountedCumulativeGain(k=3),    # DCG@3
                mz.metrics.DiscountedCumulativeGain(k=5),    # DCG@5
                mz.metrics.DiscountedCumulativeGain(k=10),   # DCG@10
                mz.metrics.DiscountedCumulativeGain(k=20),   # DCG@20
                
                # Average Precision 
                mz.metrics.AveragePrecision(),
            ]
            
            # T·∫°o model d·ª±a tr√™n t√™n
            model_name_lower = model_name.lower().replace('-', '').replace('_', '')
            
            try:
                # T·∫°o model d·ª±a tr√™n t√™n
                if 'arcii' in model_name_lower or 'arc' in model_name_lower:
                    model = mz.models.ArcII()
                elif 'matchlstm' in model_name_lower:
                    model = mz.models.MatchLSTM()
                elif 'esim' in model_name_lower:
                    model = mz.models.ESIM()
                elif 'convknrm' in model_name_lower:
                    model = mz.models.ConvKNRM()
                elif 'knrm' in model_name_lower:
                    model = mz.models.KNRM()
                elif 'mvlstm' in model_name_lower:
                    model = mz.models.MVLSTM()
                elif 'pyramid' in model_name_lower or 'matchpyramid' in model_name_lower:
                    model = mz.models.MatchPyramid()
                else:
                    model = mz.models.MatchLSTM()
                
                model.params['task'] = ranking_task
                
                # T·∫°o v√† set embedding
                try:
                    if embedding_dim == 100:
                        glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=100)
                    elif embedding_dim == 200:
                        glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=200)
                    elif embedding_dim == 300:
                        glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=300)
                    else:
                        glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=100)
                        
                    term_index = preprocessor.context['vocab_unit'].state['term_index']
                    embedding_matrix = glove_embedding.build_matrix(term_index)
                    
                    if embedding_matrix.shape[1] != embedding_dim:
                        if embedding_matrix.shape[1] < embedding_dim:
                            padding = np.zeros((embedding_matrix.shape[0], embedding_dim - embedding_matrix.shape[1]))
                            embedding_matrix = np.concatenate([embedding_matrix, padding], axis=1)
                        else:
                            embedding_matrix = embedding_matrix[:, :embedding_dim]
                    
                    l2_norm = np.sqrt((embedding_matrix * embedding_matrix).sum(axis=1))
                    l2_norm[l2_norm == 0] = 1e-8
                    embedding_matrix = embedding_matrix / l2_norm[:, np.newaxis]
                    
                    model.params['embedding'] = embedding_matrix
                    
                except Exception as e:
                    return None
                
                self._set_model_specific_params(model, model_name_lower, embedding_dim)
                
                model.build()
                model.load_state_dict(state_dict, strict=False)
                
            except Exception as e:
                return None
            
            model.to(self.device)
            model.eval()
            
            # Hi·ªÉn th·ªã th√¥ng tin GPU n·∫øu s·ª≠ d·ª•ng CUDA
            if self.device.type == "cuda":
                gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB
                gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3   # GB
            
            optimizer = torch.optim.Adam(model.parameters())
            trainer = mz.trainers.Trainer(
                model=model,
                optimizer=optimizer,
                trainloader=testloader,
                validloader=testloader,
                device=self.device
            )
            
            results = trainer.evaluate(testloader)
            
            # Chuy·ªÉn ƒë·ªïi keys t·ª´ metric objects th√†nh strings
            results_str = {}
            for metric, value in results.items():
                if hasattr(metric, '__name__'):
                    key = metric.__name__
                elif hasattr(metric, '__class__'):
                    key = metric.__class__.__name__
                else:
                    key = str(metric)
                results_str[key] = value
            
            return results_str
            
        except Exception as e:
            print(f"L·ªói khi ƒë√°nh gi√° m√¥ h√¨nh {model_name}: {e}")
            return None
    
    def _set_model_specific_params(self, model, model_name_lower: str, embedding_dim: int):
        """Thi·∫øt l·∫≠p c√°c tham s·ªë ƒë·∫∑c bi·ªát cho t·ª´ng m√¥ h√¨nh"""
        
        def safe_set_param(param_name, value):
            if param_name in model.params:
                model.params[param_name] = value
                return True
            return False
        
        if 'arcii' in model_name_lower or 'arc' in model_name_lower:
            safe_set_param('left_length', 10)
            safe_set_param('right_length', 100)
            safe_set_param('kernel_1d_count', 32)
            safe_set_param('kernel_1d_size', 3)
            safe_set_param('kernel_2d_count', [64, 64])
            safe_set_param('kernel_2d_size', [(3, 3), (3, 3)])
            safe_set_param('pool_2d_size', [(3, 3), (3, 3)])
            safe_set_param('dropout_rate', 0.3)
            
        elif 'matchlstm' in model_name_lower:
            safe_set_param('mask_value', 0)
            safe_set_param('dropout_rate', 0.2)
            safe_set_param('dropout', 0.2)
            safe_set_param('with_matching_matrix', True)
            
        elif 'esim' in model_name_lower:
            safe_set_param('mask_value', 0)
            safe_set_param('dropout', 0.2)
            safe_set_param('dropout_rate', 0.2)
            safe_set_param('hidden_size', 200)
            safe_set_param('lstm_layer', 1)
            
        elif 'convknrm' in model_name_lower:
            safe_set_param('filters', 128)
            safe_set_param('conv_activation_func', 'tanh')
            safe_set_param('max_ngram', 3)
            safe_set_param('use_crossmatch', True)
            safe_set_param('kernel_num', 11)
            safe_set_param('sigma', 0.1)
            safe_set_param('exact_sigma', 0.001)
            
        elif 'knrm' in model_name_lower:
            safe_set_param('kernel_num', 21)
            safe_set_param('sigma', 0.1)
            safe_set_param('exact_sigma', 0.001)
            
        elif 'mvlstm' in model_name_lower:
            safe_set_param('mask_value', 0)
            safe_set_param('dropout_rate', 0.2)
            safe_set_param('dropout', 0.2)
            safe_set_param('with_match_highway', True)
            
        elif 'pyramid' in model_name_lower:
            safe_set_param('mask_value', 0)
            safe_set_param('dropout_rate', 0.2)
            safe_set_param('dropout', 0.2)
            safe_set_param('dpool_size', [3, 10])
            safe_set_param('kernel_count', [32, 32])
            safe_set_param('kernel_size', [(3, 3), (3, 3)])
            safe_set_param('activation_func', 'relu')
    
    def evaluate_all_models(self, model_configs: List[Dict]) -> pd.DataFrame:
        """
        ƒê√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh v√† t·∫°o b·∫£ng so s√°nh
        
        Args:
            model_configs: List c√°c dict ch·ª©a {'name': str, 'path': str}
            
        Returns:
            DataFrame ch·ª©a k·∫øt qu·∫£ so s√°nh
        """
        print("\n" + "="*60)
        print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å T·∫§T C·∫¢ C√ÅC M√î H√åNH")
        print("="*60)
        
        all_results = {}
        
        for config in model_configs:
            name = config['name']
            path =config['path']
            
            if os.path.exists(path):
                result = self.evaluate_model(path, name)
                if result:
                    all_results[name] = result
            else:
                print(f"B·ªè qua {name}: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {path}")
        
        if not all_results:
            print("Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c ƒë√°nh gi√° th√†nh c√¥ng!")
            return pd.DataFrame()
        
        # T·∫°o DataFrame ƒë·ªÉ so s√°nh
        df = pd.DataFrame(all_results).T
        
        # S·∫Øp x·∫øp theo MAP (Mean Average Precision)
        if 'MeanAveragePrecision' in df.columns:
            df = df.sort_values('MeanAveragePrecision', ascending=False)
        
        return df
    
    def print_comparison_table(self, df: pd.DataFrame):
        """In b·∫£ng so s√°nh ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng ƒë·∫πp v·ªõi focus v√†o core IR metrics"""
        if df.empty:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã!")
            return
        
        print("\n" + "="*80)
        print("K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH - CORE INFORMATION RETRIEVAL METRICS")
        print("="*80)
        
        # Core IR metrics theo th·ª© t·ª± ∆∞u ti√™n
        core_metrics = [
            'MeanAveragePrecision',     # MAP - Most important overall metric
            'MeanReciprocalRank',       # MRR - First relevant result
            'Precision(k=1)',           # P@1 - Most strict precision  
            'Precision(k=5)',           # P@5 - Standard precision
            'Precision(k=10)',          # P@10 - Broader precision
            'NormalizedDiscountedCumulativeGain(k=5)',   # NDCG@5
            'NormalizedDiscountedCumulativeGain(k=10)',  # NDCG@10
            'DiscountedCumulativeGain(k=10)',            # DCG@10
            'AveragePrecision'          # AP - Average Precision
        ]
        
        # L·ªçc metrics c√≥ s·∫µn trong DataFrame  
        available_core_metrics = [m for m in core_metrics if m in df.columns]
        
        if available_core_metrics:
            print("CORE METRICS:")
            core_df = df[available_core_metrics]
            print(core_df.round(4))
        
        # Additional detailed metrics
        print("\n" + "-"*80)
        print("DETAILED PRECISION@K METRICS:")
        precision_metrics = [col for col in df.columns if 'Precision(k=' in col]
        if precision_metrics:
            precision_df = df[sorted(precision_metrics, key=lambda x: int(x.split('k=')[1].split(')')[0]))]
            print(precision_df.round(4))
        
        print("\n" + "-"*80)
        print("NDCG@K METRICS:")
        ndcg_metrics = [col for col in df.columns if 'NormalizedDiscountedCumulativeGain(k=' in col]
        if ndcg_metrics:
            ndcg_df = df[sorted(ndcg_metrics, key=lambda x: int(x.split('k=')[1].split(')')[0]))]
            print(ndcg_df.round(4))
        
        print("\n" + "-"*80)
        print("üìä DCG@K METRICS:")
        dcg_metrics = [col for col in df.columns if 'DiscountedCumulativeGain(k=' in col]
        if dcg_metrics:
            dcg_df = df[sorted(dcg_metrics, key=lambda x: int(x.split('k=')[1].split(')')[0]))]
            print(dcg_df.round(4))
        
        print("\n" + "="*80)
        
        # Model ranking analysis
        print("MODEL RANKING ANALYSIS:")
        if 'MeanAveragePrecision' in df.columns:
            best_map_model = df['MeanAveragePrecision'].idxmax()
            best_map_score = df.loc[best_map_model, 'MeanAveragePrecision']
            print(f"   Highest MAP: {best_map_model} ({best_map_score:.4f})")
        
        if 'MeanReciprocalRank' in df.columns:
            best_mrr_model = df['MeanReciprocalRank'].idxmax()
            best_mrr_score = df.loc[best_mrr_model, 'MeanReciprocalRank']
            print(f"   Highest MRR: {best_mrr_model} ({best_mrr_score:.4f})")
        
        if 'Precision(k=1)' in df.columns:
            best_p1_model = df['Precision(k=1)'].idxmax()
            best_p1_score = df.loc[best_p1_model, 'Precision(k=1)']
            print(f"   Highest P@1: {best_p1_model} ({best_p1_score:.4f})")
        
        if 'NormalizedDiscountedCumulativeGain(k=10)' in df.columns:
            best_ndcg10_model = df['NormalizedDiscountedCumulativeGain(k=10)'].idxmax()
            best_ndcg10_score = df.loc[best_ndcg10_model, 'NormalizedDiscountedCumulativeGain(k=10)']
            print(f"   Highest NDCG@10: {best_ndcg10_model} ({best_ndcg10_score:.4f})")
        
        # Overall performance summary
        print("\nPERFORMANCE SUMMARY:")
        if len(available_core_metrics) >= 3:
            # Calculate average rank across core metrics
            ranks = {}
            for model in df.index:
                model_ranks = []
                for metric in available_core_metrics[:5]:  # Top 5 core metrics
                    if metric in df.columns:
                        rank = df[metric].rank(ascending=False)[model]
                        model_ranks.append(rank)
                ranks[model] = np.mean(model_ranks) if model_ranks else float('inf')
            
            # Sort by average rank
            sorted_models = sorted(ranks.items(), key=lambda x: x[1])
            print("   Average Ranking (lower is better):")
            for i, (model, avg_rank) in enumerate(sorted_models, 1):
                print(f"   {i:2d}. {model:<20} (Avg Rank: {avg_rank:.2f})")
        
        print("="*80)

    
    def save_results(self, df: pd.DataFrame, filename: str = 'evaluation_results.csv'):
        """L∆∞u k·∫øt qu·∫£ v√†o file CSV"""
        if not df.empty:
            df.to_csv(filename)
            print(f"ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {filename}")

    def _get_model_callback(self, model_name_lower: str):
        """L·∫•y callback ph√π h·ª£p v·ªõi m√¥ h√¨nh"""
        if 'arcii' in model_name_lower or 'arc' in model_name_lower:
            return mz.models.ArcII.get_default_padding_callback(
                fixed_length_left=10,
                fixed_length_right=100,
                pad_word_value=0,
                pad_word_mode='pre'
            )
        elif 'matchlstm' in model_name_lower:
            return mz.models.MatchLSTM.get_default_padding_callback()
        elif 'esim' in model_name_lower:
            return mz.models.ESIM.get_default_padding_callback()
        elif 'convknrm' in model_name_lower:
            return mz.models.ConvKNRM.get_default_padding_callback()
        elif 'knrm' in model_name_lower:
            return mz.models.KNRM.get_default_padding_callback()
        elif 'mvlstm' in model_name_lower:
            return mz.models.MVLSTM.get_default_padding_callback()
        elif 'pyramid' in model_name_lower or 'matchpyramid' in model_name_lower:
            return mz.models.MatchPyramid.get_default_padding_callback()
        else:
            return mz.models.MatchLSTM.get_default_padding_callback()
    
def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='ƒê√°nh gi√° v√† so s√°nh c√°c m√¥ h√¨nh MatchZoo',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª• s·ª≠ d·ª•ng:
  python evaluate_models.py --data-pack F:\\SematicSearch\\[method]semantic_splitter
  python evaluate_models.py -d F:\\SematicSearch\\[method]semantic_splitter --output results.csv
  python evaluate_models.py --data-pack F:\\SematicSearch\\[method]semantic_splitter --models Arc-II MatchLSTM
        """
    )
    
    parser.add_argument(
        '--data-pack', '-d',
        type=str,
        required=False,
        help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a test_pack.dam'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='evaluation_results.csv',
        help='T√™n file output ƒë·ªÉ l∆∞u k·∫øt qu·∫£ (m·∫∑c ƒë·ªãnh: evaluation_results.csv)'
    )
    
    parser.add_argument(
        '--models', '-m',
        nargs='+',
        default=['Arc-II'],
        choices=['Arc-II', 'MatchLSTM', 'ESIM', 'Conv-KNRM', 'KNRM', 'Match-Pyramid', 'MVLSTM'],
        help='Danh s√°ch c√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√° (m·∫∑c ƒë·ªãnh: Arc-II)'
    )
    
    parser.add_argument(
        '--model-paths',
        nargs='+',
        help='ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ·ª©ng cho t·ª´ng m√¥ h√¨nh (theo th·ª© t·ª± v·ªõi --models)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=32,
        help='Batch size cho evaluation (m·∫∑c ƒë·ªãnh: 32)'
    )
    
    parser.add_argument(
        '--device',
        choices=['cpu', 'cuda'],
        default='cpu',
        help='Device ƒë·ªÉ ch·∫°y evaluation (m·∫∑c ƒë·ªãnh: cpu)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='In th√™m th√¥ng tin chi ti·∫øt'
    )
    
    return parser.parse_args()

def main():
    # Parse command line arguments
    args = parse_arguments()
    
    # Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a DataPack
    if hasattr(args, 'data_pack') and args.data_pack:
        data_pack_dir = args.data_pack
        print(f"S·ª≠ d·ª•ng data pack t·ª´: {data_pack_dir}")
    else:
        data_pack_dir = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a test_pack.dam: ")
    
    # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n t·ªìn t·∫°i
    if not os.path.exists(data_pack_dir):
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {data_pack_dir}")
        sys.exit(1)
    
    test_pack_path = os.path.join(data_pack_dir, 'test_pack.dam')
    if not os.path.exists(test_pack_path):
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y test_pack.dam trong {data_pack_dir}")
        sys.exit(1)
    
    # T·∫°o evaluator v·ªõi device t·ª´ CLI args
    evaluator = ModelEvaluator(data_pack_dir, batch_size=args.batch_size, device=args.device)
    
    # C·∫•u h√¨nh c√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°
    model_configs = []
    
    # ƒê∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh cho c√°c m√¥ h√¨nh
    default_model_paths = {
        'Arc-II': 'F:\\SematicSearch\\my_model\\my_model',
        'MatchLSTM': 'F:\\SematicSearch\\matchlstm_model',
        'ESIM': 'F:\\SematicSearch\\esim_model',
        'Conv-KNRM': 'F:\\SematicSearch\\conv_knrm_model',
        'KNRM': 'F:\\SematicSearch\\knrm_model',
        'Match-Pyramid': 'F:\\SematicSearch\\match_pyramid_model',
        'MVLSTM': 'F:\\SematicSearch\\mvlstm_model'
    }
    
    # X·ª≠ l√Ω model paths
    if args.model_paths:
        if len(args.model_paths) != len(args.models):
            print(f"L·ªói: S·ªë l∆∞·ª£ng ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh ({len(args.model_paths)}) kh√¥ng kh·ªõp v·ªõi s·ªë l∆∞·ª£ng m√¥ h√¨nh ({len(args.models)})")
            sys.exit(1)
        
        for model_name, model_path in zip(args.models, args.model_paths):
            model_configs.append({'name': model_name, 'path': model_path})
    else:
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh
        for model_name in args.models:
            model_path = default_model_paths.get(model_name)
            if model_path:
                model_configs.append({'name': model_name, 'path': model_path})
            else:
                print(f"Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh cho m√¥ h√¨nh {model_name}")
    
    if not model_configs:
        print("L·ªói: Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c c·∫•u h√¨nh!")
        sys.exit(1)
    
    print(f"\nC·∫•u h√¨nh ƒë√°nh gi√°:")
    print(f"   Data pack: {data_pack_dir}")
    print(f"   Output file: {args.output}")
    print(f"   Device: {args.device}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Models: {', '.join([config['name'] for config in model_configs])}")
    
    if args.verbose:
        print(f"\nChi ti·∫øt ƒë∆∞·ªùng d·∫´n m√¥ h√¨nh:")
        for config in model_configs:
            print(f"   {config['name']}: {config['path']}")
    
    # Th·ª±c hi·ªán ƒë√°nh gi√°
    results_df = evaluator.evaluate_all_models(model_configs)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    evaluator.print_comparison_table(results_df)
    
    # L∆∞u k·∫øt qu·∫£
    evaluator.save_results(results_df, args.output)

if __name__ == "__main__":
    main()
