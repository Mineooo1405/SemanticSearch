import matchzoo as mz
import torch
import os
import pandas as pd
from typing import Dict, List, Optional
import numpy as np

print("=== C√¥ng c·ª• ƒë√°nh gi√° m√¥ h√¨nh MatchZoo ===")

class ModelEvaluator:
    """L·ªõp ƒë·ªÉ ƒë√°nh gi√° v√† so s√°nh c√°c m√¥ h√¨nh MatchZoo"""
    
    def __init__(self, data_pack_dir: str):
        """
        Kh·ªüi t·∫°o evaluator
        
        Args:
            data_pack_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a test_pack.dam
        """
        self.data_pack_dir = data_pack_dir
        self.device = torch.device("cpu")
        self.test_pack_raw = None
        self.results = {}
        
        # T·∫£i test pack
        self._load_test_data()
    
    def _load_test_data(self):
        """T·∫£i d·ªØ li·ªáu test"""
        test_pack_path = os.path.join(self.data_pack_dir, 'test_pack.dam')
        if not os.path.exists(test_pack_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y test_pack.dam trong {self.data_pack_dir}")
        
        self.test_pack_raw = mz.load_data_pack(test_pack_path)
        print(f"ƒê√£ t·∫£i test data: {len(self.test_pack_raw)} samples")
    
    def evaluate_model(self, model_dir: str, model_name: str) -> Dict:
        """
        ƒê√°nh gi√° m·ªôt m√¥ h√¨nh c·ª• th·ªÉ
        
        Args:
            model_dir: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a model v√† preprocessor
            model_name: T√™n m√¥ h√¨nh ƒë·ªÉ hi·ªÉn th·ªã
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ƒë√°nh gi√°
        """
        print(f"\n--- ƒê√°nh gi√° m√¥ h√¨nh: {model_name} ---")
        
        # Ki·ªÉm tra file t·ªìn t·∫°i
        model_path = os.path.join(model_dir, 'model.pt')
        preprocessor_path = os.path.join(model_dir, 'preprocessor')
        
        if not os.path.exists(model_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y model.pt trong {model_dir}")
            return None
        
        if not os.path.exists(preprocessor_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y preprocessor trong {model_dir}")
            return None
        
        try:
            # T·∫£i preprocessor
            preprocessor = mz.load_preprocessor(preprocessor_path)
            print("‚úÖ ƒê√£ t·∫£i preprocessor")
            
            # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu test
            test_pack_processed = preprocessor.transform(self.test_pack_raw)
            print("‚úÖ ƒê√£ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu test")
            
            # T·∫°o test dataset
            testset = mz.dataloader.Dataset(
                data_pack=test_pack_processed,
                mode='point',
                batch_size=32,
                shuffle=False
            )
            
            # T·∫°o test loader
            # X√°c ƒë·ªãnh lo·∫°i m√¥ h√¨nh ƒë·ªÉ l·∫•y ƒë√∫ng callback
            if 'arcii' in model_name.lower():
                callback = mz.models.ArcII.get_default_padding_callback(
                    fixed_length_left=10,
                    fixed_length_right=100,
                    pad_word_value=0,
                    pad_word_mode='pre'
                )
            elif 'matchlstm' in model_name.lower():
                callback = mz.models.MatchLSTM.get_default_padding_callback()
            elif 'esim' in model_name.lower():
                callback = mz.models.ESIM.get_default_padding_callback()
            elif 'conv_knrm' in model_name.lower():
                callback = mz.models.ConvKNRM.get_default_padding_callback()
            elif 'knrm' in model_name.lower():
                callback = mz.models.KNRM.get_default_padding_callback()
            elif 'mvlstm' in model_name.lower():
                callback = mz.models.MVLSTM.get_default_padding_callback()
            elif 'pyramid' in model_name.lower():
                callback = mz.models.MatchPyramid.get_default_padding_callback()
            else:
                # Callback m·∫∑c ƒë·ªãnh
                callback = mz.models.MatchLSTM.get_default_padding_callback()
            
            testloader = mz.dataloader.DataLoader(
                dataset=testset,
                stage='test',
                callback=callback,
                device=self.device
            )
            print("‚úÖ ƒê√£ t·∫°o test loader")
            
            # T·∫°o ranking task v·ªõi metrics
            ranking_task = mz.tasks.Ranking()
            ranking_task.metrics = [
                mz.metrics.NormalizedDiscountedCumulativeGain(k=1),
                mz.metrics.NormalizedDiscountedCumulativeGain(k=3),
                mz.metrics.NormalizedDiscountedCumulativeGain(k=5),
                mz.metrics.NormalizedDiscountedCumulativeGain(k=10),
                mz.metrics.MeanAveragePrecision(),
                mz.metrics.Precision(k=1),
                mz.metrics.Precision(k=3),
                mz.metrics.Precision(k=5),
                mz.metrics.Precision(k=10)
            ]
            
            # T·∫°o trainer ƒë·ªÉ evaluate
            # V√¨ ch√∫ng ta kh√¥ng c√≥ model object, t·∫°o m·ªôt dummy trainer
            # Thay v√†o ƒë√≥, ta s·∫Ω t·∫°o model t·ª´ state_dict
            
            # T·∫°o model d·ª±a tr√™n t√™n
            if 'arcii' in model_name.lower():
                model = mz.models.ArcII()
            elif 'matchlstm' in model_name.lower():
                model = mz.models.MatchLSTM()
            elif 'esim' in model_name.lower():
                model = mz.models.ESIM()
            elif 'conv_knrm' in model_name.lower():
                model = mz.models.ConvKNRM()
            elif 'knrm' in model_name.lower():
                model = mz.models.KNRM()
            elif 'mvlstm' in model_name.lower():
                model = mz.models.MVLSTM()
            elif 'pyramid' in model_name.lower():
                model = mz.models.MatchPyramid()
            else:
                raise ValueError(f"Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c lo·∫°i m√¥ h√¨nh: {model_name}")
            
            # Thi·∫øt l·∫≠p tham s·ªë c∆° b·∫£n cho model
            model.params['task'] = ranking_task
            
            # T·∫£i embedding t·ª´ preprocessor context
            try:
                # L·∫•y embedding t·ª´ preprocessor context n·∫øu c√≥
                if 'embedding' in preprocessor.context:
                    model.params['embedding'] = preprocessor.context['embedding']
                else:
                    # T·∫°o embedding matrix t·ª´ term_index
                    glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=300)
                    term_index = preprocessor.context['vocab_unit'].state['term_index']
                    embedding_matrix = glove_embedding.build_matrix(term_index)
                    l2_norm = np.sqrt((embedding_matrix * embedding_matrix).sum(axis=1))
                    l2_norm[l2_norm == 0] = 1e-8
                    embedding_matrix = embedding_matrix / l2_norm[:, np.newaxis]
                    model.params['embedding'] = embedding_matrix
            except Exception as e:
                print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng th·ªÉ t·∫£i embedding: {e}")
                # T·∫°o embedding matrix m·∫∑c ƒë·ªãnh
                glove_embedding = mz.datasets.embeddings.load_glove_embedding(dimension=100)
                term_index = preprocessor.context['vocab_unit'].state['term_index']
                embedding_matrix = glove_embedding.build_matrix(term_index)
                l2_norm = np.sqrt((embedding_matrix * embedding_matrix).sum(axis=1))
                l2_norm[l2_norm == 0] = 1e-8
                embedding_matrix = embedding_matrix / l2_norm[:, np.newaxis]
                model.params['embedding'] = embedding_matrix
            
            # Thi·∫øt l·∫≠p c√°c tham s·ªë ƒë·∫∑c bi·ªát cho t·ª´ng m√¥ h√¨nh
            if 'arcii' in model_name.lower():
                model.params['left_length'] = 10
                model.params['right_length'] = 100
                model.params['kernel_1d_count'] = 32
                model.params['kernel_1d_size'] = 3
                model.params['kernel_2d_count'] = [64, 64]
                model.params['kernel_2d_size'] = [(3, 3), (3, 3)]
                model.params['pool_2d_size'] = [(3, 3), (3, 3)]
                model.params['dropout_rate'] = 0.3
            elif 'matchlstm' in model_name.lower():
                model.params['mask_value'] = 0
            elif 'esim' in model_name.lower():
                model.params['mask_value'] = 0
                model.params['dropout'] = 0.2
                model.params['hidden_size'] = 200
                model.params['lstm_layer'] = 1
            elif 'conv_knrm' in model_name.lower():
                model.params['filters'] = 128
                model.params['conv_activation_func'] = 'tanh'
                model.params['max_ngram'] = 3
                model.params['use_crossmatch'] = True
                model.params['kernel_num'] = 11
                model.params['sigma'] = 0.1
                model.params['exact_sigma'] = 0.001
            elif 'knrm' in model_name.lower():
                model.params['kernel_num'] = 21
                model.params['sigma'] = 0.1
                model.params['exact_sigma'] = 0.001
            elif 'mvlstm' in model_name.lower():
                # MVLSTM c√≥ th·ªÉ kh√¥ng c·∫ßn tham s·ªë ƒë·∫∑c bi·ªát
                pass
            elif 'pyramid' in model_name.lower():
                # MatchPyramid c√≥ th·ªÉ kh√¥ng c·∫ßn tham s·ªë ƒë·∫∑c bi·ªát
                pass
            
            # Build model
            model.build()
            
            # T·∫£i tr·ªçng s·ªë
            state_dict = torch.load(model_path, map_location=self.device)
            model.load_state_dict(state_dict)
            model.to(self.device)
            model.eval()
            
            print("‚úÖ ƒê√£ t·∫£i model v√† tr·ªçng s·ªë")
            
            # T·∫°o trainer ƒë·ªÉ evaluate
            optimizer = torch.optim.Adam(model.parameters())  # Dummy optimizer
            trainer = mz.trainers.Trainer(
                model=model,
                optimizer=optimizer,
                trainloader=testloader,  # Dummy trainloader
                validloader=testloader,
                device=self.device
            )
            
            # Th·ª±c hi·ªán evaluation
            results = trainer.evaluate(testloader)
            
            # Chuy·ªÉn ƒë·ªïi keys t·ª´ metric objects th√†nh strings
            results_str = {}
            for metric, value in results.items():
                if hasattr(metric, '__name__'):
                    key = metric.__name__
                elif hasattr(metric, '__class__'):
                    key = metric.__class__.__name__
                else:
                    key = str(metric)
                results_str[key] = value
            
            print("‚úÖ ƒê√£ ho√†n th√†nh evaluation")
            print("K·∫øt qu·∫£:")
            for metric, value in results_str.items():
                print(f"  {metric}: {value:.4f}")
            
            return results_str
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë√°nh gi√° m√¥ h√¨nh {model_name}: {e}")
            return None
    
    def evaluate_all_models(self, model_configs: List[Dict]) -> pd.DataFrame:
        """
        ƒê√°nh gi√° t·∫•t c·∫£ c√°c m√¥ h√¨nh v√† t·∫°o b·∫£ng so s√°nh
        
        Args:
            model_configs: List c√°c dict ch·ª©a {'name': str, 'path': str}
            
        Returns:
            DataFrame ch·ª©a k·∫øt qu·∫£ so s√°nh
        """
        print("\n" + "="*60)
        print("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å T·∫§T C·∫¢ C√ÅC M√î H√åNH")
        print("="*60)
        
        all_results = {}
        
        for config in model_configs:
            name = config['name']
            path ="F:/SematicSearch/trained_models/" + config['path']
            
            if os.path.exists(path):
                result = self.evaluate_model(path, name)
                if result:
                    all_results[name] = result
            else:
                print(f"‚ö†Ô∏è B·ªè qua {name}: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {path}")
        
        if not all_results:
            print("‚ùå Kh√¥ng c√≥ m√¥ h√¨nh n√†o ƒë∆∞·ª£c ƒë√°nh gi√° th√†nh c√¥ng!")
            return pd.DataFrame()
        
        # T·∫°o DataFrame ƒë·ªÉ so s√°nh
        df = pd.DataFrame(all_results).T
        
        # S·∫Øp x·∫øp theo MAP (Mean Average Precision)
        if 'MeanAveragePrecision' in df.columns:
            df = df.sort_values('MeanAveragePrecision', ascending=False)
        
        return df
    
    def print_comparison_table(self, df: pd.DataFrame):
        """In b·∫£ng so s√°nh ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng ƒë·∫πp"""
        if df.empty:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã!")
            return
        
        print("\n" + "="*80)
        print("B·∫¢NG SO S√ÅNH K·∫æT QU·∫¢ C√ÅC M√î H√åNH")
        print("="*80)
        
        # Ch·ªçn c√°c metrics quan tr·ªçng nh·∫•t ƒë·ªÉ hi·ªÉn th·ªã
        important_metrics = [
            'MeanAveragePrecision',
            'NormalizedDiscountedCumulativeGain(k=1)',
            'NormalizedDiscountedCumulativeGain(k=3)',
            'NormalizedDiscountedCumulativeGain(k=5)',
            'NormalizedDiscountedCumulativeGain(k=10)',
            'Precision(k=1)',
            'Precision(k=3)',
            'Precision(k=5)'
        ]
        
        # L·ªçc c√°c metrics c√≥ trong DataFrame
        available_metrics = [m for m in important_metrics if m in df.columns]
        
        if available_metrics:
            display_df = df[available_metrics]
            print(display_df.round(4))
        else:
            print(df.round(4))
        
        print("\n" + "="*80)
        
        # T√¨m m√¥ h√¨nh t·ªët nh·∫•t
        if 'MeanAveragePrecision' in df.columns:
            best_model = df['MeanAveragePrecision'].idxmax()
            best_score = df.loc[best_model, 'MeanAveragePrecision']
            print(f"üèÜ M√î H√åNH T·ªêT NH·∫§T: {best_model} (MAP: {best_score:.4f})")
        
        print("="*80)
    
    def save_results(self, df: pd.DataFrame, filename: str = 'evaluation_results.csv'):
        """L∆∞u k·∫øt qu·∫£ v√†o file CSV"""
        if not df.empty:
            df.to_csv(filename)
            print(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o {filename}")

def main():
    # Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a DataPack
    data_pack_dir = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a test_pack.dam: ")
    
    # T·∫°o evaluator
    evaluator = ModelEvaluator(data_pack_dir)
    
    # C·∫•u h√¨nh c√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°
    model_configs = [
        {'name': 'Arc-II', 'path': 'arcii_model'},
        {'name': 'MatchLSTM', 'path': 'matchlstm_model'},
        {'name': 'ESIM', 'path': 'esim_model'},
        {'name': 'Conv-KNRM', 'path': 'conv_knrm_model'},
        {'name': 'KNRM', 'path': 'knrm_model'},
        {'name': 'Match-Pyramid', 'path': 'match_pyramid_model'},
        {'name': 'MVLSTM', 'path': 'mvlstm_model'}
    ]
    
    # Th·ª±c hi·ªán ƒë√°nh gi√°
    results_df = evaluator.evaluate_all_models(model_configs)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    evaluator.print_comparison_table(results_df)
    
    # L∆∞u k·∫øt qu·∫£
    evaluator.save_results(results_df)
    
    print("\nüéâ Ho√†n th√†nh vi·ªác ƒë√°nh gi√°!")

if __name__ == "__main__":
    main()
